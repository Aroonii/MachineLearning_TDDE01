library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10, -1, 1)# Your code here
f <- Sin~Var
nn <- neuralnet(f, data=tr, threshold=5/1000, startweights=winit, linear.output = FALSE)
plot(nn)
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(31, -1, 1)# Your code here
nn <- neuralnet(f, data=tr, hidden=c(10), threshold=5/1000, startweights=winit, linear.output = FALSE)
plot(n)
plot(nn)
infert <- infert
View(infert)
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
###Step 2####
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
#parameters for each model
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, 0, 0)
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
scores1 = numeric(6)
for (i in 1:6){
scores1[i] = performanceScore(fold1, fold2, C[i], kernel[i], width[i])
}
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
###Step 2####
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
#parameters for each model
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, 0, 0)
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
scores1 = numeric(6)
for (i in 1:6){
scores1[i] = performanceScore(fold1, fold2, C[i], kernel[i], width[i])
}
scores2 = numeric(6)
for (i in 1:6){
scores2[i] = performanceScore(fold2, fold1, C[i], kernel[i], width[i])
}
avgScores = (scores1 + scores2)/2 #The avg performance scores for each model after cross validation
###Step 3###
model <- which.min(avgScores)
final.svm <- svm <- ksvm(type~., data=spam, kernel=kernel[model], C=C[model], kpar=list(sigma=width[model]), cross=2)
?neuralnet
??neuralnet
a(c(0.1,0.2,0.3), 5)
a = function(w, x) {
return (sum(w * x))
}
a(c(0.1,0.2,0.3), 5)
5*weights
weigths = as.vector(c(0.1,0.2,0.3))
weigths * 5
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var)) tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var)) tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
###Step 5###
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
View(tr)
View(tr)
a(weigths, 5)
weigths * 5 # ok
# returns activation of unit
z = function(a) {
return(tanh(a))
}
z(3)
###Step 4###
# takes w: a vector of weights, x: one numeric input value (Var)
# returns vector of inputs to units
a = function(w, x) {
return (w * x)
}
a(weights, 5)
a(weights, 5)
weigths = c(0.1,0.2,0.3)
weigths * 5 # ok
a(weights, 5)
a = function(w, x) {
return (w*x)
}
w = c(0.1,0.2,0.3)
w*5
a(w,5)
y = function(w, z) {
y = numeric(length(z))
return(sum(w * z))
}
w = c(0.1,0.2,0.3)
z = c(1,2,3)
w*z
1-(z^2)
z = c(1,2,3)
w_init = runif(10, -1, 1)
?runif
setwd("~/Documents/TDDE01/tenta2")
setwd("~/Documents/TDDE01/tenta2/TDDE01")
crabs = read.csv("australian-crabs.csv")
crabs = read.csv("australian-crabs.csv")
View(crabs)
plot(data.order$MET, prediction.tree, pch=21, bg="firebrick2", ylab="EX", xlab="MET", ylim
=c(min(data.order$EX),max(data.order$EX)))
View(crabs)
crabs$species = blue
crabs$species = 'blue'
crabs = read.csv("australian-crabs.csv")
crabs$species == 'blue'
crabs$species == 'Blue'
blue.crabs = crabs[species='Blue', ]
blue.crabs = crabs[crabs$species='Blue', ]
crabs = read.csv("australian-crabs.csv")
blue.crabs = crabs[crabs$species=='Blue', ]
View(blue.crabs)
orange.crabs = crabs[crabs$species=='Orange', ]
plot(orange.crabs$CV, orange.crabs$BD, pch=21, bg="firebrick2", ylab="BD", xlab="CV")
points(blue.crabs$CV, clue.crabs$BD, pch=21, bg="dodgerblue3")
plot(orange.crabs$CV, orange.crabs$BD, pch=21, bg="firebrick2", ylab="BD", xlab="CV")
points(blue.crabs$CV, blue.crabs$BD, pch=21, bg="dodgerblue3")
crabs = read.csv("australian-crabs.csv")
blue.crabs = crabs[crabs$species=='Blue', ]
orange.crabs = crabs[crabs$species=='Orange', ]
plot(orange.crabs$CV, orange.crabs$BD, pch=21, bg="firebrick2", ylab="BD", xlab="CV")
points(blue.crabs$CV, blue.crabs$BD, pch=21, bg="dodgerblue3")
crabs = read.csv("australian-crabs.csv")
blue.crabs = crabs[crabs$species=='Blue', ]
orange.crabs = crabs[crabs$species=='Orange', ]
plot(orange.crabs$CW, orange.crabs$BD, pch=21, bg="firebrick2", ylab="BD", xlab="CV")
points(blue.crabs$CW, blue.crabs$BD, pch=21, bg="dodgerblue3")
View(crabs)
View(crabs)
?naiveBayes
??naiveBayes
library(e1071)
fit.nb <- naiveBayes(species~CW + BD, data = crabs)
library(e1071)
fit.nb <- naiveBayes(species~CW + BD, data = crabs)
nb.predict <- predict(fit.nb, newdata = crabs)
CM <- table(nb.predict, crabs$species)
MCR <- 1-sum(diag(CM))/sum(CM)
print (list(CM=CM, MCR=MCR))
??Logistic
??regression
?? logistic regression
??Logistic
library(MASS)
fit.log <- polr(species ~ CW + BD, data = crabs, method="logistic")
library(MASS)
fit.log <- polr(species~CW + BD, data = crabs, method="logistic")
crabs.factors <- crabs
crabs.factors[,1] <- as.factor(crabs.factors[,1])
View(crabs.factors)
crabs.factors[,1]
fit.log <- polr(species~CW + BD, data = crabs.factors, method="logistic")
?rms
?lrm
??lrm
??rms
??rms.lrm
?rms.lrm
?lrm
??lrm
?lm
fit.log <- lm(species~CW + BD, data = crabs)
library(MASS)
crabs.factors <- crabs
crabs.factors[,1] <- as.factor(crabs.factors[,1])
fit.log <- lm(species~CW + BD, data = crabs.factors)
?Logistic
??Logistic
fit.log <- lm(species~CW + BD, data = crabs, family=Binomial)
fit.log <- lm(species~CW + BD, data = crabs, family="Binomial")
fit.log <- lm(species~CW + BD, data = crabs, family="binomial")
crabs.factors[,1] <- as.factor(crabs.factors[,1])
fit.log <- lm(species~CW + BD, data = crabs.factors, family="binomial")
fit.log <- lm(species~CW + BD, data=crabs, family="binomial")
fit.log <- glm(species~CW + BD, data=crabs, family="binomial")
###Step 1###
crabs = read.csv("australian-crabs.csv")
blue.crabs = crabs[crabs$species=='Blue', ]
orange.crabs = crabs[crabs$species=='Orange', ]
plot(orange.crabs$CW, orange.crabs$BD, pch=21, bg="firebrick2", ylab="BD", xlab="CV", main="plot 1")
points(blue.crabs$CW, blue.crabs$BD, pch=21, bg="dodgerblue3")
##Ans: yes CW and BD seems to be good predictors of the spieces
###Step 2###
library(e1071)
fit.nb <- naiveBayes(species~CW + BD, data = crabs)
nb.predict <- predict(fit.nb, newdata = crabs)
CM <- table(nb.predict, crabs$species)
MCR <- 1-sum(diag(CM))/sum(CM) # 0.395
print (list(CM=CM, MCR=MCR))
## Ans: The mcr is high, 39.5% so the quality of the classification is pretty bad.
## The assumptions of the naive bayes: assumes conditional independence for the distribution of input variables.
## This model is not appropriate for this data bescause: It seems is plot 1 that it is the combination of
## CW and BD that can say which spices a crab is.
###Step 3###
library(MASS)
crabs.factors <- crabs
crabs.factors[,1] <- as.factor(crabs.factors[,1])
fit.log <- glm(species~CW + BD, data=crabs, family="binomial")
summary(fit.log)
?decsisionboundry
??boundrt
??boundry
?glm
?boundary
?glm
fit.log$boundary
fit.log$boundary
fit.log$formula
fit.log$coefficients
plot(fit.log$coefficients)
?glm
pred.log <- predict(fit.log, crabs$species)
pred.log <- predict(fit.log, newdata=crabs)
plot(pred.log)
pred.log <- predict(fit.log, newdata=crabs, type="response")
plot(pred.log)
pred.log
?glm
require(graphics)
## example from Venables and Ripley (2002, pp. 190-2.)
ldose <- rep(0:5, 2)
numdead <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), c(6, 6)))
SF <- cbind(numdead, numalive = 20-numdead)
budworm.lg <- glm(SF ~ sex*ldose, family = binomial)
summary(budworm.lg)
plot(c(1,32), c(0,1), type = "n", xlab = "dose",
ylab = "prob", log = "x")
text(2^ldose, numdead/20, as.character(sex))
ld <- seq(0, 5, 0.1)
lines(2^ld, predict(budworm.lg, data.frame(ldose = ld,
sex = factor(rep("M", length(ld)), levels = levels(sex))),
type = "response"))
lines(2^ld, predict(budworm.lg, data.frame(ldose = ld,
sex = factor(rep("F", length(ld)), levels = levels(sex))),
type = "response"))
library(MASS)
crabs.factors <- crabs
crabs.factors[,1] <- as.factor(crabs.factors[,1])
fit.log <- glm(species~CW + BD, data=crabs, family="binomial")
pred.log <- predict(fit.log, newdata=crabs, type="response")
summary(pred.log)
plot(pred.log)
View(crabs)
crabs.inputs = data.frame(crabs[,7:8])
View(crabs.inputs)
View(crabs.inputs)
scale(crabs.inputs)
View(crabs.inputs)
crabs.inputs = scale(data.frame(crabs[,7:8]))
View(crabs.inputs)
pcd = prcomp(crabs.inputs)
screeplot(pcd)
plot(pcd$x[,1], pcd$x[,2], main="PC1 & PC2")
?prcomp
?prcomp
summary(pcd)
?prcomp
crabs.inputs = data.frame(crabs[,7:8])
pcd = prcomp(crabs.inputs, scale=TRUE)
screeplot(pcd)
summary(pcd)
pcd$center
pcd$rotation
loadings(pcd)
pcd$x
data <- data.frame(pcs, crabs$species)
pcs <- pcd$x
data <- data.frame(pcs, crabs$species)
data
fit.nb.pcs <- naiveBayes(crabs.species~PC1 + PC2, data = data)
nb.predict.pcs <- predict(fit.nb, newdata = crabs)
CM.pcs <- table(nb.predict, crabs$species)
MCR.pcs <- 1-sum(diag(CM))/sum(CM) # 0.395
print (list(CM.pcs=CM, MCR.pcs=MCR))
nb.predict.pcs <- predict(fit.nb.pcs, newdata = data)
CM.pcs <- table(nb.predict.pcs, data$crabs.species)
MCR.pcs <- 1-sum(diag(CM.pcs))/sum(CM.pcs) #
print (list(CM.pcs=CM.pcs, MCR.pcs=MCR.pcs))
###Step 1###
bank = read.csv("bank.csv")
data = read.csv("bank.csv", header=TRUE, sep=";", dec=",")
View(data)
###Step 1###
bank = read.csv("bank.csv", header=TRUE, sep=";", dec=",")
?glm
View(bank)
fit.glm <- glm(Visitors~Time, data=bank, familiy=poisson(link=log))
fit.glm <- glm(Visitors~Time, data=bank, familiy="poisson")
fit.glm <- glm(Visitors~Time, data=bank, familiy="poisson", link=log)
fit.glm <- glm(Visitors~Time, data=bank, familiy="poisson", link="log")
fit.glm <- glm(Visitors~Time, data=bank, familiy=poisson)
fit.glm <- glm(Visitors~Time, data=bank, familiy=poisson())
fit.glm <- glm(Visitors~Time, data=bank, familiy=poisson(link="log"))
fit.glm <- glm(bank$Visitors~bank$Time, familiy=poisson(link="log"))
?glmnet
??glmnet
?glm
?gl
bank$Time
fit.glm <- glm(bank$Visitors~bank$Time, familiy=poisson())
fit.glm <- glm(bank$Visitors~bank$Time, family=poisson(link="log"))
fit.glm <- glm(Visitors~Time, data=bank, family=poisson(link="log"))
View(fit.glm)
View(fit.glm)
summary(fit.glm)
anova(fit.glm)
pred <- predict(fit.glm, newdata=bank)
plot(pred)
summary(pred)
?boot
??boot
?rpois
