tree.err <- error(test.targets, tre.pred[2])
tree.pred <- predict(pruned.tree, test)
testpredval <- tree.pred[1,2]
testtargetval <- as.numeric(test$Class[1])
test.targets <- test$Class
error = function(targets, predictions) {
return (sum(targets * log(predictions) + (1-targets) * log(1-predictions)))
}
tree.err <- error(test.targets, tre.pred[2])
tree.pred <- predict(pruned.tree, test)
testpredval <- tree.pred[1,2]
testtargetval <- as.numeric(test$Class[1])
test.targets <- test$Class
error = function(targets, predictions) {
return (sum(targets * log(predictions) + (1-targets) * log(1-predictions)))
}
tree.err <- error(test.targets, tree.pred[2])
log(testpredval)
log(tree.pred[2])
tree.pred[2]
View(tree.pred)
error = function(targets, predictions) {
return (sum(targets * log(predictions) + (1-targets) * log(1-predictions)))
}
tree.err <- error(test.targets, tree.pred[,2])
log(testpredval)
log(tree.pred[,2])
View(tree.pred)
log(testpredval)
log(tree.pred[,2])
log(testpredval)*testtargetval
log(tree.pred[,2])*test.targets
log(1-testpredval)
x<- 1-testpredval
log(x)
error2 = function(targets, predictions) {
errors = numeric(length(predictions))
for (i in 1:length(predictions)) {
errors[i] = targets[i]*log(predictions[i]) + (1-targets[i]) * log(1-predictions[i])
}
return (sum(errors))
}
tree.err <- error2(test.targets, tree.pred[,2])
###Step 1###
library(tree)
crx = read.csv("crx.csv")
#View(crx)
n=dim(crx)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.8))
training=crx[id,]
test=crx[-id,]
# id = n*0.8
# training=crx[1:id,]
# test=crx[id+1:n,]
training.modified = training
training.modified[,ncol(training.modified)] = as.factor(training.modified[,ncol(training.modified)])
fit.tree <- tree(Class~.,data=training.modified)
plot(fit.tree, type=c("uniform"))
text(fit.tree, pretty=0)
training.no2=training.modified[-2,]
fit.tree.no2 <- tree(Class~.,data=training.no2)
plot(fit.tree.no2, type=c("uniform"))
text(fit.tree.no2, pretty = 0)
##Tree structure ändras ingenting???????
###Step 2###
cv.tree = cv.tree(fit.tree)
plot(cv.tree$size, cv.tree$dev, ylab="Deviance", type="b", main="cross-validation plot")
optimal.depth=cv.tree$size[which.min(cv.tree$dev)]
pruned.tree = prune.tree(fit.tree, best=optimal.depth)
plot(pruned.tree)
text(pruned.tree, pretty=0)
#Ans: The tree shuld have 6 leaves and the variables selected are A9, A3, A6, A15 and A11
###Step 3###
library(glmnet)
x_train = model.matrix( ~ .-1, training[,-16])
lasso.CV <- cv.glmnet(x_train, scale(training$Class), alpha=1, family="binomial")
coef(lasso.CV, s="lambda.min") #22st variabler
plot(lasso.CV, main="cross-validation plot")
optimal.penalty <- lasso.CV$lambda.min
lasso.CV$nzero[39] #22st variabler
# lambda > 0 is penalty factor.
# Optimal penalty parameter value is 0.01037
# Number of components selected by LASSO: 22
# For the smallest value of penalty parameter has a deviance around 0.12 higher.
# So the optimal model is better. But it is statistically more significant difference
# for higher values on penalty parameter
###Step 4###
error = function(targets, predictions) {
return (sum(targets * log(predictions) + (1-targets) * log(1-predictions)))
}
tree.pred <- predict(pruned.tree, test)
test.targets <- test$Class
tree.err <- error(test.targets, tree.pred[,2])
lasso.pred <- predict(lasso.CV, test)
glmnet.cv
?glmnet.cv
?cv.glmnet
lasso.pred <- predict(lasso.CV, as.matrix(test), s="lambda.min")
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test, s="lambda.min")
View(lasso.pred)
View(tree.pred)
View(lasso.pred)
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test)
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, as.matrix(test[,-Class]))
View(test)
x_test = as.matrix(test[, 1:15])
View(x_test)
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, x_test)
###Step 3###
library(glmnet)
x_train = model.matrix( ~ .-1, training[,-16])
lasso.CV <- cv.glmnet(x_train, as.factor(training$Class), alpha=1, family="binomial")
coef(lasso.CV, s="lambda.min") #22st variabler
plot(lasso.CV, main="cross-validation plot")
optimal.penalty <- lasso.CV$lambda.min
lasso.CV$nzero[39] #22st variabler
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, x_test)
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, as.factor(x_test))
###Step 3###
library(glmnet)
x_train = model.matrix( ~ .-1, training[,-16])
lasso.CV <- cv.glmnet(x_train, scale(as.factor(training$Class)), alpha=1, family="binomial")
coef(lasso.CV, s="lambda.min") #22st variabler
plot(lasso.CV, main="cross-validation plot")
optimal.penalty <- lasso.CV$lambda.min
lasso.CV$nzero[39] #22st variabler
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, scale(as.factor(x_test)))
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, scale(x_test))
x_test = as.matrix(test[, 1:15])
lasso.pred <- predict(lasso.CV, x_test)
###Step 3###
library(glmnet)
x_train = model.matrix( ~ .-1, training[,-16])
lasso.CV <- cv.glmnet(x_train, scale(training$Class), alpha=1, family="binomial")
coef(lasso.CV, s="lambda.min") #22st variabler
plot(lasso.CV, main="cross-validation plot")
optimal.penalty <- lasso.CV$lambda.min
lasso.CV$nzero[39] #22st variabler
# lambda > 0 is penalty factor.
# Optimal penalty parameter value is 0.01037
# Number of components selected by LASSO: 22
# For the smallest value of penalty parameter has a deviance around 0.12 higher.
# So the optimal model is better. But it is statistically more significant difference
# for higher values on penalty parameter
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test)
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test, s="lambda.min", type="response")
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test, s="lambda.min", type="class")
View(lasso.pred)
x_test = model.matrix( ~ .-1, test[,-16])
lasso.pred <- predict(lasso.CV, x_test, s="lambda.min", type="response")
View(lasso.pred)
lasso.pred
as.vector(lasso.pred)
lasso.err <- error(test.targets, as.vector(lasso.pred))
library(mboost)
bf <- read.csv2("bodyfatregression.csv") set.seed(1234567890)
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf) mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
install.packages("mboost")
library(mboost)
bf <- read.csv2("bodyfatregression.csv") set.seed(1234567890)
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf) mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf) mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
m <- blackboost(Bodyfat_percent̃Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
?mboost
View(bf)
?blackboost
?mstop
?model.weights
?cv
install.packages("kernlab")
library(kernlab)
?kernlab
data <- kernlab.spam
data <- data(spam)
View(spam)
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(kernlab)
data(spam)
View(spam)
View(spam)
??`kernelFast,besselkernel-method`
??svm
svm.gaus.1 <- ksvm(type~., data=spam, kernel="rbfdot", C=1)
?cv
?cv.ksvm
??cv.ksvm
??cv.svm
?ksvm
??kernlab
?gridSearchCV
??gridSearchCV
View(svm.gaus.1)
summary(svm.gaus.1)
View(svm.gaus.1)
svm.gaus.1$error
?ksvm
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
svm.gaus.1 <- ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2)
dim(spam[2])
prediction <- predict(svm.gaus.1, fold2[,-58])
prediction
prediction[1]
as.vector(prediction)
as.vector(prediction)[1]
svm.gaus.1 <- ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2, kpar=list(sigma=c(0.01, 0.05)))
prediction <- predict(svm.gaus.1, fold2[,-58])
svm.gaus.1 <- ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2, kpar=list(sigma=0.01))
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
svm.gaus.1.01 <- ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2, kpar=list(sigma=0.01))
prediction <- predict(svm.gaus.1.01, fold2[,-58])
table(prediction, fold2[,58])
CM <- table(prediction, fold2[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
c(1,"kernel",0.05)
?data.frame
data.frame(colnames(cols), c("rbfdot",1,0.01), c("rbfdot",1,0.05), c("rbfdot",5,0.01))
cols <- c("kernel","C","width")
data.frame(colnames(cols), c("rbfdot",1,0.01), c("rbfdot",1,0.05), c("rbfdot",5,0.01))
L3 <- LETTERS[1:3]
fac <- sample(L3, 10, replace = TRUE)
(d <- data.frame(x = 1, y = 1:10, fac = fac))
## The "same" with automatic column names:
data.frame(1, 1:10, sample(L3, 10, replace = TRUE))
cols <- c("kernel","C","width")
data.frame(col.names=cols, c("rbfdot",1,0.01), c("rbfdot",1,0.05), c("rbfdot",5,0.01))
cols <- c("kernel","C","width")
data.frame(col.names=cols, c("rbfdot",1,0.01), c("rbfdot",1,0.05), c("rbfdot",5,0.01))
?ksvm
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, NULL, NULL)
data.frame(C,kernel,width)
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, 0, 0)
data.frame(C,kernel,width)
modelParams <- data.frame(C,kernel,width)
modelParams[1]
modelParams[1,]
modelParams[1,]$C
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
performanceScore(fold1, fold2, 1, "rbfdot", 0.01)
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, 0, 0)
modelParams <- data.frame(C,kernel,width)
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
performanceScore(fold1, fold2, 1, "rbfdot", 0.01)
scoresRun1 = numeric(6)
scores = numeric(6)
for (i in 1:6){
scores[i] = performanceScore(fold1, fold2, modelParams[i,]$kernel, modelParams[i,]$C, modelParams[i,]$width)
}
modelParams[1,]$width
performanceScore(fold1,fold2,1,"rbfdot",0.00)
performanceScore(fold1,fold2,1,"rbfdot",0.01)
debugonce(performanceScore)
performanceScore(fold1,fold2,1,"rbfdot",0.01)
debugonce(performanceScore)
performanceScore(fold1,fold2,1,"rbfdot",0.00)
debugonce(performanceScore)
performanceScore(fold1,fold2,1,"rbfdot",modelParams[6,]$width)
performanceScore = function(traindata, testdata, C, kernel, width) {
print(width)
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
debugonce(performanceScore)
performanceScore(fold1,fold2,1,"rbfdot",modelParams[6,]$width)
debugonce(performanceScore)
performanceScore(fold1,fold2,1,"rbfdot",modelParams[1,]$width)
scores = numeric(6)
for (i in 1:6){
print(modelParams[i,]$kernel)
print(modelParams[i,]$C)
print(modelParams[i,]$width)
scores[i] = performanceScore(fold1, fold2, modelParams[i,]$kernel, modelParams[i,]$C, modelParams[i,]$width)
}
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
scores = numeric(6)
for (i in 1:6){
print(modelParams[i,]$kernel)
print(modelParams[i,]$C)
print(modelParams[i,]$width)
scores[i] = performanceScore(fold1, fold2, modelParams[i,]$kernel, modelParams[i,]$C, modelParams[i,]$width)
}
scores = numeric(6)
for (i in 1:6){
print(as.string(modelParams[i,]$kernel))
print(modelParams[i,]$C)
print(modelParams[i,]$width)
scores[i] = performanceScore(fold1, fold2, modelParams[i,]$kernel, modelParams[i,]$C, modelParams[i,]$width)
}
print(modelParams[i,]$kernel)
as.character(modelParams[1,]$kernel)
scores = numeric(6)
for (i in 1:6){
print(modelParams[i,]$kernel)
print(modelParams[i,]$C)
print(modelParams[i,]$width)
scores[i] = performanceScore(fold1, fold2, as.character(modelParams[i,]$kernel), modelParams[i,]$C, modelParams[i,]$width)
}
performanceScore = function(traindata, testdata, C, kernel, width) {
print(C)
print(kernel)
print(width)
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
scores = numeric(6)
for (i in 1:6){
scores[i] = performanceScore(fold1, fold2, as.character(modelParams[i,]$kernel), modelParams[i,]$C, modelParams[i,]$width)
}
scores = numeric(6)
for (i in 1:6){
scores[i] = performanceScore(fold1, fold2, as.character(modelParams[i,]$kernel), modelParams[i,]$C, modelParams[i,]$width)
}
kernel
kernel[1]
scores = numeric(6)
for (i in 1:6){
scores[i] = performanceScore(fold1, fold2, C[i], kernel[i], width[i])
}
scores = numeric(6)
for (i in 1:6){
scores[i] = performanceScore(fold1, fold2, C[i], kernel[i], width[i])
}
###Step 1###
library(mboost)
bf <- read.csv2("bodyfatregression.csv")
set.seed(1234567890)
# gradient boosting for optimizing arbitary loss functions where regression trees are utiized as base-learners
m <- blackboost(Bodyfat_percent~Waist_cm+Weight_kg, data=bf)
mstop(m)
cvf <- cv(model.weights(m), type="kfold")
cvm <- cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(kernlab)
data(spam)
n=dim(spam)[1]
set.seed(1234567890)
id=sample(1:n, floor(n*0.5))
fold1=spam[id,]
fold2=spam[-id,]
#parameters for each model
C <- c(1,5,1,5,1,5)
kernel <- c("rbfdot", "rbfdot", "rbfdot", "rbfdot", "vanilladot", "vanilladot")
width <- c(0.01, 0.01, 0.05, 0.05, 0, 0)
performanceScore = function(traindata, testdata, C, kernel, width) {
if (width == 0.00){
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2)
} else {
svm <- ksvm(type~., data=traindata, kernel=kernel, C=C, cross=2, kpar=list(sigma=width))
}
pred <- predict(svm, testdata[,-58])
CM <- table(pred, testdata[,58])
MCR <- 1-sum(diag(CM))/sum(CM)
return (MCR)
}
scores1 = numeric(6)
for (i in 1:6){
scores1[i] = performanceScore(fold1, fold2, C[i], kernel[i], width[i])
}
scores2 = numeric(6)
for (i in 1:6){
scores2[i] = performanceScore(fold2, fold1, C[i], kernel[i], width[i])
}
avgScores = (scores1 + scores2)/2
which.min(avgScores)
modelNo <- which.min(avgScores)
model <- which.min(avgScores)
final.svm <- svm <- ksvm(type~., data=spam, kernel=kernel[model], C=C[model], kpar=list(sigma=width[model]), cross=2)
